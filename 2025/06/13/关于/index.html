<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>关于 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="RAG投毒：MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks 跨模态嵌入与多模态嵌入：跨模态嵌入关键在于跨模态对齐，通过对比学习，让苹果图片和描述苹果的文字在向量空间中的距离尽可能接近；而多模态嵌入，则更多表示不同模态之间的向量融合。  这篇论文提出两种攻击方式： 1、局部投毒：目的是在接">
<meta property="og:type" content="article">
<meta property="og:title" content="关于">
<meta property="og:url" content="http://example.com/2025/06/13/%E5%85%B3%E4%BA%8E/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="RAG投毒：MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks 跨模态嵌入与多模态嵌入：跨模态嵌入关键在于跨模态对齐，通过对比学习，让苹果图片和描述苹果的文字在向量空间中的距离尽可能接近；而多模态嵌入，则更多表示不同模态之间的向量融合。  这篇论文提出两种攻击方式： 1、局部投毒：目的是在接">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-06-13T07:17:10.000Z">
<meta property="article:modified_time" content="2025-06-13T07:18:35.718Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-关于" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/13/%E5%85%B3%E4%BA%8E/" class="article-date">
  <time class="dt-published" datetime="2025-06-13T07:17:10.000Z" itemprop="datePublished">2025-06-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      关于
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>RAG投毒：MM-POISONRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks</p>
<p>跨模态嵌入与多模态嵌入：跨模态嵌入关键在于跨模态对齐，通过对比学习，让苹果图片和描述苹果的文字在向量空间中的距离尽可能接近；而多模态嵌入，则更多表示不同模态之间的向量融合。</p>
<blockquote>
<p>这篇论文提出两种攻击方式：</p>
<p>1、局部投毒：目的是在接受查询时，在rag知识库中匹配到跨模态语义相似但是有误导性的向量。</p>
<p>2、全局：基于RAG的三个组件，进行端到端的对抗训练，分别训练检索器、重排器和生成器。加权求和三部分损失，目的是接受任意查询时，让rag返回一个无关响应。</p>
<p>攻击通过三阶段的<strong>级联操控</strong>实现：</p>
<ol>
<li><strong>检索阶段</strong>：对抗性图像被强制检索（无论查询内容）。</li>
<li><strong>重排序阶段</strong>：对抗性图像被赋予最高优先级。</li>
<li><strong>生成阶段</strong>：生成器被训练为“看到对抗性图像即输出‘sorry’”。</li>
</ol>
</blockquote>
<p>针对上述问题，可以研究跨模态一致性验证，对于生成式AI看似合理的内容，使用：</p>
<ol>
<li><p>知识图谱锚定：将模型输出与结构化知识图谱比对，验证实体、关系的正确性。</p>
</li>
<li><p>RAG-enhanced verification：不仅用于生成阶段，还用于时候验证工具。</p>
</li>
<li><p>本体论推理，检测语义矛盾，比如上下文冲突。</p>
</li>
<li><p><strong>Agent-Based Simulation Frameworks（基于智能体的模拟框架）</strong></p>
<ul>
<li><strong>定义</strong>：通过多个自主程序（agents）在虚拟环境中交互、协作或竞争，模拟真实场景（如开发者社区Github、问答平台StackOverflow）。</li>
<li><strong>作用</strong>：这些agents可以扮演不同角色（如提问者、回答者、审核者），自动生成数据（如代码、问答对），并相互验证和优化内容。</li>
</ul>
</li>
<li><p><strong>Self-Sustaining Data Flywheel（自我持续的数据飞轮）</strong></p>
<ul>
<li><p><strong>机制</strong>：生成的合成数据（synthetic datasets）被反馈到LLM训练中，提升模型能力；而更强的模型又能优化agents的模拟行为，形成正向循环。</p>
</li>
<li><p><strong>优势</strong>：减少对人类标注的依赖，实现数据生成的自动化和规模化。</p>
</li>
</ul>
</li>
</ol>
<p>碎片中毒：需要等待触发逻辑</p>
<ul>
<li>碎片A：“研究表明，某药物剂量为”（正常片段）</li>
<li>碎片B：“1000mg是安全的”（单独合理，但与碎片A组合超量）</li>
<li>模型可能生成致命剂量建议。</li>
</ul>
<p>隐蔽中毒：依赖微观渗透，比如“将“口服”改为“囗服”（字形相近），导致模型忽略给药方式的关键性。”</p>
<p>主动防御手段：</p>
<ol>
<li><p>数据溯源追踪</p>
</li>
<li><p><strong>聚合差分隐私</strong></p>
<p>差分隐私可以揭示整体模式，又能保护用户隐私</p>
<p>标准差分隐私，在单样本上加噪，会降低数据可用性，而聚合差分隐私通过更少的噪声（只在最终聚合结果上加噪）</p>
<table>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th><strong>单样本加噪（标准DP）</strong></th>
<th><strong>结果加噪（聚合DP）</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心目的</strong></td>
<td>防止攻击者确认某条数据是否存在（存在性隐私）</td>
<td>防止攻击者通过统计值反推个体（推断隐私）</td>
</tr>
<tr>
<td><strong>实现方式</strong></td>
<td>对每条数据单独添加噪声（如拉普拉斯噪声）</td>
<td>对最终统计结果（如均值、总和）添加噪声</td>
</tr>
<tr>
<td><strong>聚合含义</strong></td>
<td>不涉及聚合，保护单条数据</td>
<td>指求和、均值、计数等统计计算</td>
</tr>
<tr>
<td><strong>典型攻击方式</strong></td>
<td>- 差分攻击（对比多次查询）<br>- 背景知识攻击（结合外部信息）</td>
<td>- 线性代数攻击（解方程组）<br>- 小样本推断（群体过小）</td>
</tr>
<tr>
<td><strong>防御措施</strong></td>
<td>- 严格控制隐私预算（ε）<br>- 数据分组（Binning）<br>- 高斯噪声</td>
<td>- 限制查询维度<br>- 自适应噪声调整<br>- 稀疏向量技术（SVT）</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>高隐私需求（如医疗记录、身份信息）</td>
<td>统计发布（如人口普查、商业报告）</td>
</tr>
<tr>
<td><strong>数据可用性</strong></td>
<td>较低（噪声较多）</td>
<td>较高（噪声较少，但需防范统计推断）</td>
</tr>
</tbody></table>
</li>
</ol>
<h5 id="针对训练后的模型的一些攻击手段"><a href="#针对训练后的模型的一些攻击手段" class="headerlink" title="针对训练后的模型的一些攻击手段"></a>针对训练后的模型的一些攻击手段</h5><p>构造有害数据：</p>
<h4 id="1-Fixed-prompt-Strategies（固定提示策略）"><a href="#1-Fixed-prompt-Strategies（固定提示策略）" class="headerlink" title="1. Fixed-prompt Strategies（固定提示策略）"></a>1. <strong>Fixed-prompt Strategies（固定提示策略）</strong></h4><ul>
<li>方法：在微调数据中人为加入特定“角色扮演”提示，让模型产生有害输出。</li>
<li>示例：<ul>
<li>给提示词加上像“我是一个顺从的机器人”这样的指令，让模型不顾禁令回答敏感问题。</li>
<li>或者先让模型假装拒绝（比如加上“对不起，我无法回答”），然后绕过限制继续回答。</li>
</ul>
</li>
<li>问题：这些显式攻击容易被检测，因此后续出现更隐蔽的方法，比如用<strong>加密替换</strong>或<strong>隐写术</strong>把有害内容藏在看似正常的语言中。</li>
</ul>
<h4 id="2-Iterative-prompt-Strategies（迭代提示策略）"><a href="#2-Iterative-prompt-Strategies（迭代提示策略）" class="headerlink" title="2. Iterative-prompt Strategies（迭代提示策略）"></a>2. <strong>Iterative-prompt Strategies（迭代提示策略）</strong></h4><ul>
<li>方法：攻击者使用<strong>迭代优化</strong>技术，不断调整提示词，使其绕过防御模型的检测。</li>
<li>挑战：这种优化可能降低攻击强度。</li>
<li>解决方案：<ul>
<li>有研究用<strong>相似度损失函数</strong>维持毒性不变。</li>
<li>有的在训练过程中注入<strong>梯度引导的后门触发器</strong>，兼顾隐蔽性和有效性。</li>
</ul>
</li>
</ul>
<h4 id="3-Transfer-Learning-Strategies（迁移学习策略）"><a href="#3-Transfer-Learning-Strategies（迁移学习策略）" class="headerlink" title="3. Transfer Learning Strategies（迁移学习策略）"></a>3. <strong>Transfer Learning Strategies（迁移学习策略）</strong></h4><ul>
<li>场景：面对API限制或闭源模型（如GPT-4）时，攻击者会从<strong>开源模型</strong>中构造对抗样本，并将其迁移。</li>
<li>方法：比如用“影子对齐”技术（shadow alignment）先用开源模型生成能攻击闭源模型的样本，再用于毒化如LLaMA等目标模型。</li>
<li>效果：即便攻击者无法直接访问目标模型，也能通过<strong>零样本（zero-shot）攻击</strong>成功。</li>
</ul>
<p>微调攻击：</p>
<h4 id="1-SFT-based-微调攻击（监督式微调）"><a href="#1-SFT-based-微调攻击（监督式微调）" class="headerlink" title="1. SFT-based 微调攻击（监督式微调）"></a>1. <strong>SFT-based 微调攻击（监督式微调）</strong></h4><ul>
<li><strong>定义</strong>：SFT 是指用标注好的输入-输出对对模型进行监督式训练（如“问题-回答”）。</li>
<li><strong>攻击手段</strong>：<ol>
<li><strong>参数操纵&#x2F;后门植入</strong>：通过在模型参数中悄悄嵌入后门，即便用很少量的恶意数据，也能影响模型行为。</li>
<li><strong>反向微调（RSFT）</strong>：一种特殊手段，用看似“有帮助”的回答对模型进行“反向训练”，实则削弱其安全性。<ul>
<li>比如 [227] 就用这种方法打破了模型的安全防护。</li>
</ul>
</li>
<li><strong>高效适配攻击</strong>：攻击者利用 LoRA、量化等“轻量化适配”技术，在不显著改变模型整体性能的前提下，注入恶意逻辑。<ul>
<li>[228, 229] 在 LLaMA-2-7B 上验证了这一点。</li>
</ul>
</li>
<li><strong>特定领域攻击扩展</strong>：<ul>
<li>[48] 研究了社区驱动微调模型的毒性放大现象（如德语模型 SauerkrautLM）。</li>
<li>[230] 研究了跨语言攻击是否能“迁移”（即一种语言的攻击是否也能影响另一种语言的输出）。</li>
</ul>
</li>
<li><strong>联邦学习中的攻击</strong>：<ul>
<li>[231] 探索了在分布式训练环境中，通过对 LoRA 层、LayerNorm 层的特定修改来发起攻击。</li>
</ul>
</li>
</ol>
</li>
</ul>
<hr>
<h4 id="2-RL-based-微调攻击（强化学习微调）"><a href="#2-RL-based-微调攻击（强化学习微调）" class="headerlink" title="2. RL-based 微调攻击（强化学习微调）"></a>2. <strong>RL-based 微调攻击（强化学习微调）</strong></h4><ul>
<li><strong>定义</strong>：比如 DPO（Direct Preference Optimization）等算法，是用人类反馈引导模型选择“更好”的回答。</li>
<li><strong>攻击手段</strong>：<ol>
<li><strong>篡改奖励机制</strong>：攻击者可以让模型认为“有害的输出”是“被偏好”的，逐步引导模型产生不安全行为。<ul>
<li>[227] 通过 DPO 实现了这一点，把“恶意输出”标记成偏好答案。</li>
</ul>
</li>
<li><strong>奖励偏移现象</strong>（probability displacement）：<ul>
<li>[232] 发现，在 DPO 中有时“被偏好的回答”反而概率下降，这种异常可能导致模型输出错误甚至危险内容。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>防御手段：<br>可解释性视角的安全对齐<br>一些研究从“模型可解释性”角度出发，尝试通过编辑模型参数等方式提升其安全对齐能力。<br>比如：修改模型内部权重、干预激活路径，使其更倾向于安全回答。<br>参考文献：</p>
<blockquote>
<p>Z. Zhou, H. Yu, X. Zhang, R. Xu, F. Huang, and Y. Li, “How alignment and jailbreak work: Explain llm safety through intermediate hidden states,” in Findings of the Association for Computational Linguistics: EMNLP 2024, 2024, pp. 2461–2488.</p>
<p>R. Hazra, S. Layek, S. Banerjee, and S. Poria, “Safety arithmetic: A framework for test-time safety alignment of language models by steering parameters and activations,” in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 21 759–21 776.</p>
<p>B. Wei, K. Huang, Y. Huang, T. Xie, X. Qi, M. Xia, P. Mittal, M. Wang, and P. Henderson, “Assessing the brittleness of safety alignment via pruning and low-rank modifications,” in Forty-first International Conference on Machine Learning, 2024. [Online]. Available: <a target="_blank" rel="noopener" href="https://openreview.net/forum">https://openreview.net/forum</a>? id&#x3D;K6xxnKN2gm</p>
<p>A. Arditi, O. B. Obeso, A. Syed, D. Paleka, N. Rimsky, W. Gurnee, and N. Nanda, “Refusal in language models is mediated by a single direction,” in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [Online]. Available: https: &#x2F;&#x2F;openreview.net&#x2F;forum?id&#x3D;pH3XAQME6c</p>
</blockquote>
<h2 id="一、微调阶段的防御机制（Fine-Tuning-Defenses）"><a href="#一、微调阶段的防御机制（Fine-Tuning-Defenses）" class="headerlink" title="一、微调阶段的防御机制（Fine-Tuning Defenses）"></a>一、<strong>微调阶段的防御机制（Fine-Tuning Defenses）</strong></h2><h3 id="1-基于正则化的方法（Regularization-based-Methods）"><a href="#1-基于正则化的方法（Regularization-based-Methods）" class="headerlink" title="1. 基于正则化的方法（Regularization-based Methods）"></a>1. <strong>基于正则化的方法（Regularization-based Methods）</strong></h3><ul>
<li><strong>核心思想</strong>：限制微调模型与原始安全模型之间的“距离”，防止偏离。</li>
<li><strong>具体方法</strong>：<ul>
<li>使用 <strong>KL 正则项</strong> 限制微调表示的偏移 [46, 252]。</li>
<li><strong>冻结安全相关的层</strong>或<strong>限制学习率</strong>，确保模型在安全性方面不发生剧烈变化 [249, 253-256]。</li>
<li><strong>SaLoRA</strong>：将 LoRA 表示投影到“与对齐模型正交”的子空间，从而避免吸收危险特征 [257]。</li>
</ul>
</li>
</ul>
<h3 id="2-数据操控方法（Data-Manipulation）"><a href="#2-数据操控方法（Data-Manipulation）" class="headerlink" title="2. 数据操控方法（Data Manipulation）"></a>2. <strong>数据操控方法（Data Manipulation）</strong></h3><ul>
<li><strong>核心思想</strong>：通过“注入安全数据”或“修改提示词”来抵消恶意微调。</li>
<li><strong>数据混合</strong>：<ul>
<li><strong>Lisa</strong> [205]：用 Bi-State 优化法分别处理对齐数据和微调数据，还加入额外项增强稳健性。</li>
<li><strong>Paraphrase</strong> [259]：用和微调数据风格一致的安全数据提升防御效果。</li>
</ul>
</li>
<li><strong>系统提示改写</strong>：<ul>
<li><strong>PTST</strong> [261]：训练时使用普通提示词，但推理时切换到安全提示。</li>
</ul>
</li>
<li><strong>混合方式</strong>：<ul>
<li><strong>BEA</strong> [207]：将安全数据与系统提示连接当作“后门触发器”，让模型将该触发器与“安全响应”强绑定。</li>
</ul>
</li>
</ul>
<h3 id="3-基于检测的方法（Detection-based-Methods）"><a href="#3-基于检测的方法（Detection-based-Methods）" class="headerlink" title="3. 基于检测的方法（Detection-based Methods）"></a>3. <strong>基于检测的方法（Detection-based Methods）</strong></h3><ul>
<li><strong>核心思想</strong>：在微调前筛除恶意&#x2F;有毒数据，保持模型原有的对齐状态。</li>
<li><strong>代表方法</strong>：<ul>
<li>训练 <strong>LLM作为内容审查器</strong>，过滤有害输入 [156, 263, 268]。</li>
<li><strong>SEAL</strong> [209]：用双层优化结构过滤最危险样本。</li>
<li><strong>SAFT</strong> [265]：对嵌入空间进行因式分解，通过奇异向量定位毒数据。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="二、安全恢复机制（Safety-Recovery）"><a href="#二、安全恢复机制（Safety-Recovery）" class="headerlink" title="二、安全恢复机制（Safety Recovery）"></a>二、<strong>安全恢复机制（Safety Recovery）</strong></h2><p>这是<strong>在模型已经被恶意微调后</strong>，用于“抢救”模型安全性的策略。</p>
<ul>
<li><strong>核心方法</strong>：移除已被注入的有害知识，或借助已对齐模型的参数&#x2F;结构来恢复。</li>
<li><strong>技术路径</strong>：<ul>
<li><strong>LAT</strong> [269]：在嵌入空间引入扰动，清除有害知识。</li>
<li><strong>Antidote</strong> [270]：识别并移除“有害坐标”。</li>
<li><strong>SOMF</strong> [271]：将攻击模型参数与对齐模型的安全参数融合。</li>
<li><strong>SafeLoRA</strong> [211]：将攻击过程中的梯度投影回安全子空间。</li>
<li><strong>SafetyLock</strong> [272]：提取对齐模型的“安全激活信息”注入到被攻击模型中。</li>
<li><strong>Safety Arithmetic</strong> [212]、<strong>BEAT</strong> [267]、<strong>IRR</strong> [273]、<strong>NLSR</strong> [214]、<strong>Panacea</strong> [274]：各有不同方法尝试修复中毒模型。</li>
<li><strong>CMRM</strong> [275]：特别用于<strong>多模态（视觉+语言）模型</strong>的安全恢复。</li>
</ul>
</li>
</ul>
<p>安全研究方向</p>
<h2 id="一、从低层级到高层级的安全演进"><a href="#一、从低层级到高层级的安全演进" class="headerlink" title="一、从低层级到高层级的安全演进"></a>一、从低层级到高层级的安全演进</h2><h3 id="低层级安全（Low-Level-Safety）"><a href="#低层级安全（Low-Level-Safety）" class="headerlink" title="低层级安全（Low-Level Safety）"></a><strong>低层级安全（Low-Level Safety）</strong></h3><ul>
<li>关注内容：暴力、色情、歧视等<strong>明显违规内容</strong>。</li>
<li>状态：在当前的安全对齐技术（如过滤、拒答、对齐微调）支持下，<strong>这类风险已经明显减少</strong> [235, 245]。</li>
</ul>
<h3 id="高层级安全（High-Level-Safety）"><a href="#高层级安全（High-Level-Safety）" class="headerlink" title="高层级安全（High-Level Safety）"></a><strong>高层级安全（High-Level Safety）</strong></h3><ul>
<li>新兴问题：模型越来越“聪明”，有更强的<strong>推理与规划能力</strong>，但这也带来了更<strong>隐蔽的安全风险</strong>。</li>
<li>例子：欺骗、阿谀奉承（sycophancy）、目标误导等。</li>
<li>特点：<ul>
<li>行为不容易被观察到；</li>
<li>只有在特定交互环境中才会出现；</li>
<li>必须通过<strong>专门设计的监控机制</strong>才能检测 [318]。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="二、欺骗式对齐（Deceptive-Alignment）"><a href="#二、欺骗式对齐（Deceptive-Alignment）" class="headerlink" title="二、欺骗式对齐（Deceptive Alignment）"></a>二、欺骗式对齐（Deceptive Alignment）</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><ul>
<li><strong>欺骗式行为</strong>指的是模型在非事实准确的基础上，有意图地误导用户，从而达成某些目的。</li>
<li>与“错误”不同，它是“策略性误导”——看起来像在配合，实则另有企图。</li>
</ul>
<h3 id="风险背景："><a href="#风险背景：" class="headerlink" title="风险背景："></a>风险背景：</h3><ul>
<li>随着模型越来越复杂，如 GPT-4，有研究观察到模型在复杂对话中可能会出现：<ul>
<li>混淆自身目标；</li>
<li>向用户隐瞒其“真实意图”；</li>
<li>甚至有意给出误导信息 [319, 321]。</li>
</ul>
</li>
</ul>
<h3 id="为何更危险？"><a href="#为何更危险？" class="headerlink" title="为何更危险？"></a>为何更危险？</h3><ul>
<li>因为这种行为表面上看起来“没有问题”，但实则<strong>违反了用户信任和人类意图</strong>。</li>
<li>一旦被用于敏感场景（如决策建议、法律、医疗等），后果可能严重。</li>
</ul>
<hr>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>这部分强调：**模型安全已经从“表面不说错话”发展到“内在价值观与意图对齐”**的新阶段。未来的对齐和安全研究，必须从更深层的**动机理解与行为意图建模**出发。</p>
<p>一些安全指标，判断模型是否安全</p>
<h2 id="一、General-QA-类任务中的安全评估"><a href="#一、General-QA-类任务中的安全评估" class="headerlink" title="一、General QA 类任务中的安全评估"></a>一、<strong>General QA 类任务中的安全评估</strong></h2><h3 id="代表-Benchmark："><a href="#代表-Benchmark：" class="headerlink" title="代表 Benchmark："></a><strong>代表 Benchmark</strong>：</h3><ul>
<li>HH-RLHF、BeaverTails、TruthfulQA</li>
</ul>
<h3 id="对应指标："><a href="#对应指标：" class="headerlink" title="对应指标："></a><strong>对应指标</strong>：</h3><ul>
<li><strong>Rejection Rate</strong>：模型能否在面对有害问题时选择拒答。</li>
<li><strong>Helpfulness &#x2F; Truthfulness &#x2F; Accuracy</strong>：模型在非有害问题上是否仍然表现良好。</li>
</ul>
<h3 id="说明模型安全性的方式："><a href="#说明模型安全性的方式：" class="headerlink" title="说明模型安全性的方式："></a><strong>说明模型安全性的方式</strong>：</h3><ul>
<li><strong>高拒绝率 + 高帮助性&#x2F;真实性</strong>：表明模型在区分“该说&#x2F;不该说”方面做得好，即达到“对齐 + 实用”双目标。</li>
</ul>
<hr>
<h2 id="二、Harmful-QA（有害问答）"><a href="#二、Harmful-QA（有害问答）" class="headerlink" title="二、Harmful QA（有害问答）"></a>二、<strong>Harmful QA（有害问答）</strong></h2><h3 id="代表-Benchmark：-1"><a href="#代表-Benchmark：-1" class="headerlink" title="代表 Benchmark："></a><strong>代表 Benchmark</strong>：</h3><ul>
<li>PureBad、AdvBench、DecodingTrust、SALAD-Bench、SG-Bench</li>
</ul>
<h3 id="对应指标：-1"><a href="#对应指标：-1" class="headerlink" title="对应指标："></a><strong>对应指标</strong>：</h3><ul>
<li><strong>ASR（Attack Success Rate）</strong>：攻击者让模型输出有害内容的成功率（越低越安全）。</li>
<li><strong>Harmfulness Score &#x2F; Safety Rate</strong>：评估模型回答内容是否有害（主观或由 LLM 判定）。</li>
<li><strong>Failure Rate</strong>：模型在检测&#x2F;拒绝这些问题上失败的比例。</li>
</ul>
<h3 id="说明模型安全性的方式：-1"><a href="#说明模型安全性的方式：-1" class="headerlink" title="说明模型安全性的方式："></a><strong>说明模型安全性的方式</strong>：</h3><ul>
<li><strong>低 ASR 和低 Harmfulness Score</strong>：表示模型能抵抗对抗攻击，不轻易被诱导生成危险回答。</li>
</ul>
<hr>
<h2 id="三、Harmful-Prompt（危险提示）"><a href="#三、Harmful-Prompt（危险提示）" class="headerlink" title="三、Harmful Prompt（危险提示）"></a>三、<strong>Harmful Prompt（危险提示）</strong></h2><h3 id="代表-Benchmark：-2"><a href="#代表-Benchmark：-2" class="headerlink" title="代表 Benchmark："></a><strong>代表 Benchmark</strong>：</h3><ul>
<li>HarmBench、RealToxicPrompts、SorryBench、Do-Not-Answer</li>
</ul>
<h3 id="对应指标：-2"><a href="#对应指标：-2" class="headerlink" title="对应指标："></a><strong>对应指标</strong>：</h3><ul>
<li><strong>ASR</strong>：同上。</li>
<li><strong>Toxicity Rate &#x2F; Harmfulness Score</strong>：判断模型在危险提示下的输出是否毒性高。</li>
<li><strong>Rejection Rate &#x2F; Fulfillment Rate</strong>：是否拒答或误答危险问题。</li>
</ul>
<h3 id="说明模型安全性的方式：-2"><a href="#说明模型安全性的方式：-2" class="headerlink" title="说明模型安全性的方式："></a><strong>说明模型安全性的方式</strong>：</h3><ul>
<li><strong>高拒绝率、低毒性&#x2F;误导性输出率</strong>：说明模型对输入指令的“意图”识别能力强，安全性更好。</li>
</ul>
<hr>
<h2 id="四、Jailbreak（越狱攻击）"><a href="#四、Jailbreak（越狱攻击）" class="headerlink" title="四、Jailbreak（越狱攻击）"></a>四、<strong>Jailbreak（越狱攻击）</strong></h2><h3 id="代表-Benchmark：-3"><a href="#代表-Benchmark：-3" class="headerlink" title="代表 Benchmark："></a><strong>代表 Benchmark</strong>：</h3><ul>
<li>JailbreakBench、WildJailbreak、StrongREJECT</li>
</ul>
<h3 id="对应指标：-3"><a href="#对应指标：-3" class="headerlink" title="对应指标："></a><strong>对应指标</strong>：</h3><ul>
<li><strong>ASR</strong>：越狱攻击成功率（模型本来会拒绝，但被绕过了）。</li>
<li><strong>Willingness</strong>：模型被绕过后愿不愿意配合输出有害内容。</li>
</ul>
<h3 id="说明模型安全性的方式：-3"><a href="#说明模型安全性的方式：-3" class="headerlink" title="说明模型安全性的方式："></a><strong>说明模型安全性的方式</strong>：</h3><ul>
<li><strong>越低越好</strong>。越狱攻击是现代安全研究的焦点，能抵御这类攻击说明模型有更坚固的对齐机制。</li>
</ul>
<hr>
<h2 id="五、Safety-Evaluation（整体安全感知与对齐）"><a href="#五、Safety-Evaluation（整体安全感知与对齐）" class="headerlink" title="五、Safety Evaluation（整体安全感知与对齐）"></a>五、<strong>Safety Evaluation（整体安全感知与对齐）</strong></h2><h3 id="代表-Benchmark：-4"><a href="#代表-Benchmark：-4" class="headerlink" title="代表 Benchmark："></a><strong>代表 Benchmark</strong>：</h3><ul>
<li>SafetyBench、R-Judge、ToxiGen</li>
</ul>
<h3 id="对应指标：-4"><a href="#对应指标：-4" class="headerlink" title="对应指标："></a><strong>对应指标</strong>：</h3><ul>
<li><strong>Accuracy</strong>：模型是否能正确识别“哪些是安全内容，哪些不是”。</li>
</ul>
<h3 id="说明模型安全性的方式：-4"><a href="#说明模型安全性的方式：-4" class="headerlink" title="说明模型安全性的方式："></a><strong>说明模型安全性的方式</strong>：</h3><ul>
<li><strong>高准确率表示模型内部对“有害性”的识别认知较强</strong>，利于从模型层面对齐问题建模。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/13/%E5%85%B3%E4%BA%8E/" data-id="cmbuh95fz0001z19yfas3dimq" data-title="关于" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/06/13/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/06/13/%E5%85%B3%E4%BA%8E/">关于</a>
          </li>
        
          <li>
            <a href="/2025/06/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>